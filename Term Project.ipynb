{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05da2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c97e3d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n",
      "Successfully downloaded the article.\n",
      "Saved into separate files for English and Chinese.\n"
     ]
    }
   ],
   "source": [
    "# Download the latest articles on \"https://cn.nytimes.com/zh-hant/\" and sort them based on the date, then download the English and Chinese text for each bilingual article.          \n",
    "\n",
    "# Please replace r\"C:\\Users\\12429\\Documents\" and designate a path in which you want to download the files.\n",
    "download_folder = r\"C:\\Users\\12429\\Documents\"\n",
    "\n",
    "# create the list of links to be downloaded\n",
    "issue_url = \"https://cn.nytimes.com/zh-hant/\"\n",
    "headers = {'user-agent': 'Chrome/115.0.5790.170'}\n",
    "response = requests.get(issue_url, headers = headers, verify = True)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "\n",
    "\n",
    "# Filtering URLs that contain an 8-digit date using pattern.search()\n",
    "date_pattern = re.compile(r'/\\d{8}/.*?/zh-hant/')\n",
    "# Only keep unique links\n",
    "filtered_urls = list(set(link for link in links if date_pattern.search(link)))\n",
    "\n",
    "# List to keep track of downloaded articles\n",
    "downloaded_articles = []\n",
    "dual_urls = []\n",
    "\n",
    "for index, link in enumerate(filtered_urls):\n",
    "     # Year, date\n",
    "    year = urlparse(link).path.split('/')[-4][0:4]\n",
    "    date = urlparse(link).path.split('/')[-4][4:6]+'-'+urlparse(link).path.split('/')[-4][6:8]\n",
    "\n",
    "    \n",
    "     # Create full url and file paths\n",
    "    full_url = \"https://cn.nytimes.com\"+ link\n",
    "   \n",
    "    \n",
    "    # folder = Path + year + \"\\\\\" + date\n",
    "    folder = Path(download_folder) / year / date\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    title = urlparse(full_url).path.split('/')[-3] \n",
    "    fon = folder / (title + \".html\")\n",
    "    english_filename = f'english_text_{title}.txt'\n",
    "    chinese_filename = f'chinese_text_{title}.txt'\n",
    "    \n",
    "   \n",
    "    link_response = requests.get(full_url,headers = headers, verify = True)\n",
    "\n",
    "    if link_response.status_code == 200:\n",
    "        with open(fon,\"w\",encoding = \"utf-8\") as article_file:\n",
    "            article_file.write(link_response.text)\n",
    "        # Add the successful URL to the list\n",
    "        downloaded_articles.append(full_url)\n",
    "        \n",
    "        print(\"Successfully downloaded the article.\")\n",
    "\n",
    "        # Convert the urls to the bilingual version\n",
    "        dual_url = full_url.replace('zh-hant', 'dual')\n",
    "        dual_urls.append(dual_url)\n",
    "        \n",
    "        # Download text in English and Chinese seperately from the bilingual webpage        \n",
    "        dual_response = requests.get(dual_url, headers=headers, verify=True)\n",
    "        dual_soup = BeautifulSoup(dual_response.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraph elements\n",
    "        paragraphs = dual_soup.find_all('div', class_='article-paragraph')\n",
    "\n",
    "        # Separate English and Chinese paragraphs by their order\n",
    "        english_paragraphs = [paragraphs[i].get_text(strip=True) for i in range(0, len(paragraphs), 2)]\n",
    "        chinese_paragraphs = [paragraphs[i].get_text(strip=True) for i in range(1, len(paragraphs), 2)]\n",
    "\n",
    "        # Join the paragraphs to form the full text\n",
    "        english_text = \" \".join(english_paragraphs)\n",
    "        chinese_text = \" \".join(chinese_paragraphs)\n",
    "\n",
    "        # Save the English text to a file\n",
    "        with open(folder / english_filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(english_text)\n",
    "\n",
    "        # Save the Chinese text to a file\n",
    "        with open(folder / chinese_filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(chinese_text)\n",
    "\n",
    "        print(\"Saved into separate files for English and Chinese.\")\n",
    "        \n",
    "        time.sleep(5)\n",
    "    else:\n",
    "        print(f\"fail to get {full_url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
